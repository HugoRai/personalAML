{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a85423",
   "metadata": {},
   "source": [
    "# Tabular Reinforcement Learning assignment (18 p)\n",
    "\n",
    "## Setup instructions\n",
    "\n",
    "This folder contains a `uv.lock` file which allows you to run `uv sync` (assuming you have [uv](https://docs.astral.sh/uv/) installed) to create a virtual environment at `.venv`. Alternatively, you can create a virtual environment manually and install the correct packages by running `pip install -r requirements.txt`.\n",
    "\n",
    "Before running code cells in this notebook, ensure that you have selected the correct python kernel, located at `.venv`.\n",
    "\n",
    "## Introduction\n",
    "This assignment will give you practice in creating your own reinforcement learning agents in tabular environments.\n",
    "\n",
    "The assignment will use the [gymnasium](https://gymnasium.farama.org/) structure, \n",
    "and I suggest you familiarize yourselves with this before getting started. In particular, the documentation about the\n",
    "[step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step) method might be useful.\n",
    "\n",
    "The assignment is split into the following tasks:\n",
    "1. Understand the code for an MDP environment.\n",
    "2. Implement MDP simulation logic.\n",
    "3. Plot results from simulations with a random policy.\n",
    "4. Answer some theoretical questions about Q-learning, SARSA and ESARSA.\n",
    "5. Calculate optimal Q-values by using value iteration.\n",
    "6. Learn optimal Q-values by using Q-learning.\n",
    "\n",
    "Each task will have a brief description, followed by specific instructions under each subtask.\n",
    "\n",
    "Feel free to slightly modify the provided code if needed. For example, if your agents don't behave as expected, it might be useful to add logging statements during the simulation. However, deviating *too* much from the provided structure makes it more difficult to grade and is therefore discouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40156642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# Imports. You should not have to change anything here, but you are allowed to do so.\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from typing import final, override, Protocol, Callable, TypeVar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1f5da",
   "metadata": {},
   "source": [
    "## Task 1: Interpreting MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09ad90",
   "metadata": {},
   "source": [
    "Here is the environment that we will solve in this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@final\n",
    "class RiverSwim(gym.Env):\n",
    "    def __init__(self, n_states=6, small=5/1000, large=1, seed: int | None = None):\n",
    "        self.n_states = n_states\n",
    "        self.small = small  # payout for 'backwards' action\n",
    "        self.large = large  # payout at end of chain for 'forwards' action\n",
    "        self.state = 0  # Start at beginning of the chain\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "\n",
    "\n",
    "    @override\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if action == 0:  # Go left\n",
    "            if self.state == 0:\n",
    "                reward = self.small\n",
    "            else:\n",
    "                self.state -= 1\n",
    "        else: # Go right\n",
    "            if self.state == 0:\n",
    "                self.state = int(self.np_random.choice([self.state, self.state + 1], p=[0.4, 0.6]))\n",
    "            elif self.state < self.n_states - 1:\n",
    "                self.state = int(self.np_random.choice([self.state-1, self.state, self.state + 1], p=[0.05, 0.6, 0.35]))\n",
    "            else: # self.state == self.n_states - 1\n",
    "                self.state = int(self.np_random.choice([self.state-1, self.state, 0], p=[0.4, 0.4, 0.2]))\n",
    "                if self.state == 0:\n",
    "                    reward = self.large\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        return self.state, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    @override\n",
    "    def reset(self, seed: int | None = None, options: dict | None = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = 0\n",
    "        info = {}\n",
    "        return self.state, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e1f09",
   "metadata": {},
   "source": [
    "To check your understanding of this MDP, you should now draw a model of it. You can use any tool you like, e.g. pen and paper, draw.io, etc. Please make it readable.\n",
    "\n",
    "For each transition, annotate the transition probability and reward. If the reward is zero, you can leave it out.\n",
    "\n",
    "Here is an example for a different environment:\n",
    "\n",
    "![](images/MDP-example.png)\n",
    "\n",
    "Remember to double check that your transition probabilities (for a fixed action) sum to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ae5e7",
   "metadata": {},
   "source": [
    "### Task 3.1 (2 p)\n",
    "Draw the MDP diagram of the `RiverSwim` environment when `n_states=4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec76c19",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0e6ab",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e8c6c",
   "metadata": {},
   "source": [
    "### Task 3.2 (0.5 p)\n",
    "Is this an episodic or continuing environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed501f8",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8999d",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb69c2",
   "metadata": {},
   "source": [
    "## Task 2: MDP simulation logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518bdc5",
   "metadata": {},
   "source": [
    "Once the environment is defined, we can move on to interacting with it. Before we bother with acting *optimally*, let us first ensure that we can act randomly.\n",
    "\n",
    "The MDP simulation code must assume an interface for acquiring actions from the agent, and for training the agent. For this we will use the very simple `RiverSwimAgent` protocol below. You will first create a purely random agent that implements this, and later on a smarter agent as well.\n",
    "\n",
    "`RiverSwimAgentT` is defined as a type variable bound to `RiverSwimAgent`. You do not have to understand this fully and it is merely included for type annotation purposes. Only remember that whenever you read `RiverSwimAgentT` you should interpret it as \"any agent that implements the `RiverSwimAgent` protocol\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7963381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RiverSwimAgent(Protocol):\n",
    "    \"\"\"An agent for the RiverSwim environment. Observations and actions are always integers.\"\"\"\n",
    "\n",
    "    def act(self, observation: int) -> int:\n",
    "        \"\"\"Given an observation, return an action.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def update(self, observation: int, action: int, reward: float, next_observation: int) -> None:\n",
    "        \"\"\"Update the agent's internal state given a (S,A,R,S) transition.\"\"\"\n",
    "        ...\n",
    "\n",
    "RiverSwimAgentT = TypeVar(\"RiverSwimAgentT\", bound=RiverSwimAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d59c20",
   "metadata": {},
   "source": [
    "Some scaffolding has been provided but you have to implement the core logic yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c959383",
   "metadata": {},
   "source": [
    "### Task 2.1 (2 p)\n",
    "Implement the `riverswim_simulation` function. Make sure that you use all the parameters of the function. Notably, `callback` will later allow us to visualize an agent's training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caca52d",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def riverswim_simulation(\n",
    "        env: RiverSwim,\n",
    "        agent: RiverSwimAgentT,\n",
    "        n_steps: int,\n",
    "        # Callback takes (agent, state, action, reward) as input\n",
    "        # Useful for visualizing agent properties during training\n",
    "        callback: Callable[[RiverSwimAgentT, int, int, float], None],\n",
    "        callback_interval: int # in number of steps, how often to call the callback\n",
    "    ) -> tuple[list[int], list[int], list[float]]:\n",
    "    \"\"\"Train `agent` in `env` for `n_steps` steps.\n",
    "\n",
    "    Args:\n",
    "        env: The RiverSwim environment.\n",
    "        agent: The agent to train.\n",
    "        n_steps: Number of steps to train for.\n",
    "        callback: A function to call every `callback_interval` steps. Will be called with (agent, state, action, reward).\n",
    "        callback_interval: How often to call the callback.\n",
    "\n",
    "    Return lists of states, actions and rewards.\"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # States should contain S_0, S_1, ..., S_n_steps\n",
    "    # Actions should contain A_0, A_1, ..., A_{n_steps-1}\n",
    "    # Rewards should contain R_1, R_2, ..., R_{n_steps}\n",
    "    assert len(states) == n_steps + 1\n",
    "    assert len(actions) == n_steps\n",
    "    assert len(rewards) == n_steps\n",
    "\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733b7d3",
   "metadata": {},
   "source": [
    "### Task 2.2 (0.5 p)\n",
    "Implement the `RandomRiverSwimAgent` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299f232",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRiverSwimAgent(RiverSwimAgent):\n",
    "    def __init__(self, seed: int | None = None):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99c418",
   "metadata": {},
   "source": [
    "## Task 3: Gathering data and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0f3c6",
   "metadata": {},
   "source": [
    "Now that we have an environment, an agent (albeit a very trivial one), and modelled the interaction between them, let's start gathering some data.\n",
    "\n",
    "Since RL is stochastic by nature, it is not enough to train an agent a single time if we want a reliable measure of the algorithm's performance. We have to do it multiple times so we can calculate both the average performance and variance.\n",
    "\n",
    "You are given this function which uses the `riverswim_simulation` function to train an agent multiple times. It returns numpy arrays of states, actions and rewards. Note that it only uses the callback during the final training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9951d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_training_runs(\n",
    "        env_factory: Callable[[], RiverSwim],\n",
    "        agent_factory: Callable[[], RiverSwimAgentT],\n",
    "        n_steps: int,\n",
    "        n_runs: int,\n",
    "        callback: Callable[[RiverSwimAgentT, int, int, float], None],\n",
    "        callback_interval: int\n",
    "    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, RiverSwimAgentT]:\n",
    "    \"\"\"Train an agent multiple times in the RiverSwim environment.\n",
    "\n",
    "    Args:\n",
    "        env_factory: A function that creates a new RiverSwim environment.\n",
    "        agent_factory: A function that creates a new RiverSwimAgent.\n",
    "        n_steps: Number of steps to train each agent for.\n",
    "        n_runs: Number of training runs to perform.\n",
    "        callback: A function to forward to `riverswim_calculation` during the final training run.\n",
    "        callback_interval: Forward to `riverswim_calculation`, only relevant for the final training run.\n",
    "    Returns:\n",
    "        all_states: Array of shape (n_runs, n_steps + 1) containing states\n",
    "        all_actions: Array of shape (n_runs, n_steps) containing actions\n",
    "        all_rewards: Array of shape (n_runs, n_steps) containing rewards\n",
    "        final_agent: The agent from the final training run\n",
    "    \"\"\"\n",
    "    all_states = np.zeros((n_runs, n_steps + 1), dtype=int)\n",
    "    all_actions = np.zeros((n_runs, n_steps), dtype=int)\n",
    "    all_rewards = np.zeros((n_runs, n_steps), dtype=float)\n",
    "\n",
    "    env = env_factory()\n",
    "    agent: RiverSwimAgentT | None = None\n",
    "\n",
    "    for run in tqdm(range(n_runs), desc=\"Training runs\"):\n",
    "        agent = agent_factory()\n",
    "        if run == n_runs - 1:\n",
    "            states, actions, rewards = riverswim_simulation(env, agent, n_steps, callback, callback_interval)\n",
    "        else:\n",
    "            # For all but the last run, we do not need the callback\n",
    "            states, actions, rewards = riverswim_simulation(env, agent, n_steps, lambda agent,state,action,reward: None, n_steps + 1)\n",
    "        all_states[run] = states\n",
    "        all_actions[run] = actions\n",
    "        all_rewards[run] = rewards\n",
    "\n",
    "    assert  agent is not None\n",
    "\n",
    "    return all_states, all_actions, all_rewards, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c6f59",
   "metadata": {},
   "source": [
    "### Task 3.1 (0.5 p)\n",
    "\"Train\" the `RandomRiverSwimAgent` on the `RiverSwim` environment (`n_states=6`) 10 times for 100k steps by using the `multiple_training_runs` function. Since the agent does not learn anything, do not provide a callback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f7975",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b0216",
   "metadata": {},
   "source": [
    "### Task 3.2 (1 p)\n",
    "Implement `smooth_and_statistics` for smoothing an array along the time dimension and then calculating the mean and standard deviation across the run dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd2a43",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c420141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_and_statistics(array: np.ndarray, window: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Apply moving average and calculate mean and std across runs.\n",
    "\n",
    "    Args:\n",
    "        array: Array of shape (n_runs, n_steps) containing data.\n",
    "        window: Window size for moving average.\n",
    "    Returns:\n",
    "        mean: Mean across runs after smoothing, shape (n_steps-window+1,)\n",
    "        std: Standard deviation across runs after smoothing, shape (n_steps-window+1,)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # Hint: np.convolve might be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08438e",
   "metadata": {},
   "source": [
    "### Task 3.3 (1.5 p)\n",
    "Use a window size of 1k and call `smooth_and_statistics` on the rewards acquired by the random agent. Plot the mean (using `plot`) and standard deviation (using `fill_between`) of reward in a figure.\n",
    "\n",
    "Also state whether the results are reasonable to you or not. Is the reward scale what you expected from a random agent? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e513a3",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa250b",
   "metadata": {},
   "source": [
    "Since the random policy is likely to get the small reward 0.005 with a considerable probability, but also 0 more often than not, it makes sense that the reward is between 0 and 0.005."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5925bf",
   "metadata": {},
   "source": [
    "## Task 4: Method background\n",
    "\n",
    "Now that we know that our simulation works, we are of course interested in solving it, which is the domain of RL. Most tabular RL methods are *value*-based, meaning that they are based on estimations of the expected return, either for states or state-action pairs. Two of the most well-known methods are Q-learning and SARSA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4148c",
   "metadata": {},
   "source": [
    "### Task 4.1 (0.5 p)\n",
    "Write down the update equation for Q-learning and SARSA. Let $\\alpha$ denote the update step size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9464f7",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e428a",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c15591",
   "metadata": {},
   "source": [
    "### Task 4.2 (0.5 p)\n",
    "For each algorithm, draw the backup diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3da16",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d4eeb",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb04512",
   "metadata": {},
   "source": [
    "### Task 4.3 (0.25 p)\n",
    "For each algorithm, state whether $q_\\pi$ (action value for the behavior policy) or $q_*$ (action value for the optimal policy) is estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac65c1a",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3b320",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ff2c1",
   "metadata": {},
   "source": [
    "### Task 4.4 (0.25 p)\n",
    "For each algorithm, state whether it can be used off-policy or has to be on-policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70074a6c",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7da8f",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6921b",
   "metadata": {},
   "source": [
    "### Task 4.5 (1 p)\n",
    "\n",
    "Give an example of when each method would be preferable over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993c63f",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd4b10",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3c394",
   "metadata": {},
   "source": [
    "## Task 5: Value iteration\n",
    "\n",
    "For the `RiverSwim` environment, we will use Q-learning as a way of learning a good policy.\n",
    "\n",
    "However, since we know the exact dynamics of this environment, it can be helpful to calculate the true optimal Q-values *before* we start training our agent. This way, we will immediately see whether our agent works or not.\n",
    "\n",
    "*Value iteration* is a way of calculating optimal values (either state or state-action) from knowledge about an environments transition dynamics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d976f6",
   "metadata": {},
   "source": [
    "### Task 5.1 (0.5 p)\n",
    "Write down the update equation for *action value iteration*. The LHS should be $q_{k+1}(s,a)$ and the RHS should be a function of\n",
    "- $p(s',r|s,a)$: The probability of reaching state $s'$ and obtaining reward $r$ given that we choose action $a$ in state $s$.\n",
    "- $r$: The reward.\n",
    "- $\\gamma$: The discount factor.\n",
    "- $q_k(s',a')$: The previous Q-value estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4622b1",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fab7f",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dceaa",
   "metadata": {},
   "source": [
    "### Task 5.2 (1.5 p)\n",
    "Implement `qvalue_iteration`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3910fb",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qvalue_iteration(env_dynamics: dict, gamma: float, threshold: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Perform Q-value iteration given transition probabilities and rewards.\n",
    "\n",
    "    Args:\n",
    "        env_dynamics (dict): A dictionary containing environment dynamics. It has the form\n",
    "            {\n",
    "                state: {\n",
    "                    action: [(next_state, reward, probability), ...],\n",
    "                    ...\n",
    "                },\n",
    "                ...\n",
    "            }\n",
    "        Note that for a given (state, action) pair, the sum of probabilities of all possible next states should be 1.\n",
    "        gamma (float): The discount factor.\n",
    "        threshold (float): A small threshold for determining convergence.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D array of shape (n_states, n_actions) representing the optimal Q-values.\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef1ec8",
   "metadata": {},
   "source": [
    "### Task 5.3 (1 p)\n",
    "- Define a `env_dynamics` dictionary according to the `RiverSwim` environment.\n",
    "- Calculate the optimal Q-values when $\\gamma=0.99$\n",
    "- Visualize them in a 2x6 image. The exact Q-values should be annotated as text within each pixel. Since we will also want to visualize the Q-values during Q-learning, it is recommended that you create a separate function for this.\n",
    "\n",
    "One helpful way of understanding the format of `env_dynamics` is\n",
    "- Start at some state (a large circle in the MDP diagram). This is the first index.\n",
    "- Choose one of the actions (represented by small black dots in the MDP diagram). This is the second index.\n",
    "- Each tuple $(s',r,p)$ corresponds directly to one of the arrows that originates from this state-action pair.\n",
    "\n",
    "As a sanity check, all optimal Q-values are between $3$ and $5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e7e14",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dynamics = {}\n",
    "\n",
    "r_small = 5/1000\n",
    "r_large = 1\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7b88d",
   "metadata": {},
   "source": [
    "## Task 6: Q-learning\n",
    "\n",
    "Let us now train the Q-learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ce751",
   "metadata": {},
   "source": [
    "### Task 6.1 (1 p)\n",
    "Implement `QLearningRiverSwimAgent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf8ced",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa216dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningRiverSwimAgent(RiverSwimAgent):\n",
    "    def __init__(self, n_states: int, gamma: float, eps: float = 0.1, lr: float = 1e-3, init_val: float = 0, seed: int | None = None):\n",
    "        # TODO\n",
    "\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb0be0",
   "metadata": {},
   "source": [
    "### Task 6.2 (0.5 p)\n",
    "Train two different `QLearningRiverSwimAgent`s on the `RiverSwim` environment (`n_states=6`) 10 times for 100k steps by using the `multiple_training_runs` function. One of them should initialize the Q-values to 0, and the other to 5, which is larger than the true Q-values. This sort of initialization is called \"optimistic\".\n",
    "\n",
    "Furthermore, use $\\gamma=0.99$, the same value that we used for value iteration. You are free to choose `eps` and `lr` as you see fit, although it should be the same for both agents. \n",
    "\n",
    "Visualize each agent's Q-values every 10k steps for the last training run, using the `callback` and `callback_interval` arguments of `multiple_training_runs`. Also save the MSE value between the agent's Q-values and the true Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db05890",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0324c",
   "metadata": {},
   "source": [
    "### Task 6.3 (1 p)\n",
    "- In one figure, plot the smoothed rewards of both Q-learning agents together with the smoothed rewards from the random policy agent. Use a window size of 1k. Just as before, you should plot the mean reward as a line and standard deviations as filled areas.\n",
    "- In a second figure, plot the Q-value MSE for both agents with log scale for the y-axis.\n",
    "- In a third figure, plot action histograms for all three agents. Only consider the last 1000 steps, but across all training runs.\n",
    "\n",
    "At least one of the agents should attain MSE below $0.1$. If not, go back to the previous task and train again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b2cca",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07dcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078e858",
   "metadata": {},
   "source": [
    "### Task 6.4 (2 p)\n",
    "Which of the two Q-learning agents performs the best, and why?\n",
    "\n",
    "Your answer is expected to be more detailed than \"because of X\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741afe7",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da731f",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
